# Time Reclamation App Configuration

app:
  name: "Time Reclamation App"
  version: "1.0.0"
  description: "Reclaim time wasted on social media by getting curated summaries instead of endless scrolling"
  author: "Time Reclamation Team"

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console_colors: true

# Platform configurations
platforms:
  youtube:
    enabled: true
    channels:
      - name: "TechWizard9000"
        scrap: true
        url: "https://www.youtube.com/@TechWizard9000"
        max_videos: 10
        language: "fr"
        cache_folder: "cache_data/youtube_transcripts/TechWizard9000"
        
        # Summary configuration (optional)
        # Uncomment and configure to enable audio summaries for this channel
        # summary:
        #   enabled: true
        #   llm_provider: "claude_assistant"  # Optional: specific LLM instance (auto-selects if not specified)
        #   tts_provider: "kokoro_english"    # Optional: specific TTS instance (auto-selects if not specified)
        #   notification_provider: "personal_bot"  # Optional: specific notification instance (auto-selects if not specified)
        #   system_prompt: |  # Optional: custom system prompt for summaries
        #     You are a podcast host creating an engaging audio summary.
        #     Convert the transcript into a conversational, easy-to-listen format.
        #     Focus on key insights and make it sound natural for audio playback.
        #     Keep it concise but informative (3-5 minutes when spoken).
      
      # Add more channels as needed
      # - name: "Another Channel"
      #   scrap: true
      #   url: "https://www.youtube.com/@anotherchannel"
      #   max_videos: 5
      #   language: "en"
      #   cache_folder: "cache_data/youtube_transcripts/another_channel"
  reddit:
    enabled: true
  twitter:
    enabled: true

# Database configuration
database:
  path: "cache_data/state.db"
  auto_create: true

# Notification settings
notifications:
  providers:
    # Example Telegram instances - you can have multiple instances with different configurations
    - name: "work_bot"
      type: "telegram"
      enabled: false  # Set to true and configure tokens to enable
      config:
        # Telegram Bot Token (get from @BotFather on Telegram)
        # How to get your bot token:
        # 1. Open Telegram and search for @BotFather
        # 2. Start a chat and send /newbot
        # 3. Follow the instructions to create your bot
        # 4. Copy the token provided by BotFather
        # 5. Replace YOUR_BOT_TOKEN_HERE with your actual token
        bot_token: "YOUR_WORK_BOT_TOKEN_HERE"
        
        # Chat ID where messages will be sent
        # How to get your chat ID:
        # 1. Start a chat with your bot on Telegram
        # 2. Send any message to the bot
        # 3. Open this URL in your browser (replace YOUR_BOT_TOKEN with your actual token):
        #    https://api.telegram.org/botYOUR_BOT_TOKEN/getUpdates
        # 4. Look for "chat":{"id": YOUR_CHAT_ID} in the response
        # 5. Replace YOUR_CHAT_ID_HERE with the actual chat ID (it's usually a number)
        chat_id: "YOUR_WORK_CHAT_ID_HERE"
        
        # Connection settings
        timeout_seconds: 30
        retry_attempts: 3
    
    - name: "personal_bot"
      type: "telegram"
      enabled: false  # Set to true and configure tokens to enable
      config:
        bot_token: "YOUR_PERSONAL_BOT_TOKEN_HERE"
        chat_id: "YOUR_PERSONAL_CHAT_ID_HERE"
        timeout_seconds: 15
        retry_attempts: 2
    
    # You can add more instances as needed
    # - name: "alerts_bot"
    #   type: "telegram"
    #   enabled: true
    #   config:
    #     bot_token: "YOUR_ALERTS_BOT_TOKEN_HERE"
    #     chat_id: "YOUR_ALERTS_CHAT_ID_HERE"
    #     timeout_seconds: 10
    #     retry_attempts: 1

# LLM configuration
llm:
  providers:
    # Example LlamaCpp instances - you can have multiple instances with different configurations
    - name: "general_assistant"
      type: "llamacpp"
      enabled: false  # Set to true and configure model path to enable
      config:
        # Path to your GGUF model file
        # Download models from Hugging Face (e.g., TheBloke repositories)
        # Example: wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
        model_path: "/path/to/your/model.gguf"
        
        # Model configuration
        context_size: 4096      # Context window size
        gpu_layers: 33          # Number of layers to offload to GPU (0 = CPU only, -1 = all layers)
        
        # Generation parameters
        generation_config:
          max_tokens: 8000      # Maximum tokens to generate
          temperature: 0.7      # Creativity level (0.0 = deterministic, 1.0 = very creative)
          top_p: 0.9           # Nucleus sampling parameter
          top_k: 40            # Top-k sampling parameter
          repeat_penalty: 1.1   # Penalty for repetition
          stop: ["<|file_separator|>"]  # Stop sequences
        
        # Default system prompt for this instance
        default_system_prompt: "You are a helpful, harmless, and honest AI assistant. You provide clear, accurate, and concise responses to user questions."
        
        # Chat template (optional, uses default if not specified)
        chat_template: |
          <|system|>
          {system_prompt}
          <|user|>
          {user_prompt}
          <|assistant|>
    
    - name: "code_expert"
      type: "llamacpp"
      enabled: false  # Set to true and configure model path to enable
      config:
        # Use a code-specialized model for this instance
        model_path: "/path/to/code-model.gguf"
        context_size: 8192
        gpu_layers: 0  # CPU only for this instance
        generation_config:
          max_tokens: 4000
          temperature: 0.3  # Lower temperature for more focused code generation
          top_p: 0.95
          top_k: 50
          repeat_penalty: 1.1
        default_system_prompt: "You are an expert programmer. Provide clean, efficient, and well-documented code. Explain your solutions clearly."
    
    # You can add more instances as needed
    # - name: "creative_writer"
    #   type: "llamacpp"
    #   enabled: true
    #   config:
    #     model_path: "/path/to/creative-model.gguf"
    #     context_size: 4096
    #     gpu_layers: 20
    #     generation_config:
    #       max_tokens: 6000
    #       temperature: 0.9  # Higher temperature for creativity
    #       top_p: 0.8
    #       top_k: 30
    #     default_system_prompt: "You are a creative writing assistant. Help users craft engaging stories, poems, and creative content."
    
    # Example Anthropic Claude instances - you can have multiple instances with different configurations
    - name: "claude_assistant"
      type: "anthropic"
      enabled: false  # Set to true and configure API key to enable
      config:
        # Anthropic API Key (get from https://console.anthropic.com/)
        # How to get your API key:
        # 1. Go to https://console.anthropic.com/
        # 2. Sign up or log in to your account
        # 3. Navigate to API Keys section
        # 4. Create a new API key
        # 5. Replace YOUR_ANTHROPIC_API_KEY_HERE with your actual key
        api_key: "YOUR_ANTHROPIC_API_KEY_HERE"
        
        # Model configuration
        model: "claude-haiku-4-5"  # Latest Claude Haiku 4.5 - fastest small model
        max_tokens: 4000                     # Maximum tokens to generate
        temperature: 0.7                     # Creativity level (0.0 = deterministic, 1.0 = very creative)
        
        # Default system prompt for this instance
        default_system_prompt: "You are Claude, a helpful AI assistant created by Anthropic. You provide clear, accurate, and thoughtful responses."
    
    - name: "claude_coder"
      type: "anthropic"
      enabled: false  # Set to true and configure API key to enable
      config:
        api_key: "YOUR_ANTHROPIC_API_KEY_HERE"
        model: "claude-sonnet-4-5"  # Latest Claude Sonnet 4.5 for coding tasks
        max_tokens: 4000
        temperature: 0.3  # Lower temperature for more focused code generation
        default_system_prompt: "You are Claude, an expert programming assistant. Provide clean, efficient, and well-documented code with clear explanations."
    
    # Example OpenAI GPT instances - you can have multiple instances with different configurations
    - name: "gpt_assistant"
      type: "openai"
      enabled: false  # Set to true and configure API key to enable
      config:
        # OpenAI API Key (get from https://platform.openai.com/api-keys)
        # How to get your API key:
        # 1. Go to https://platform.openai.com/api-keys
        # 2. Sign up or log in to your account
        # 3. Click "Create new secret key"
        # 4. Copy the generated key
        # 5. Replace YOUR_OPENAI_API_KEY_HERE with your actual key
        api_key: "YOUR_OPENAI_API_KEY_HERE"
        
        # Model configuration
        model: "gpt-5"          # Latest GPT-5 - smartest and most capable model
        max_tokens: 4000         # Maximum tokens to generate
        temperature: 0.7         # Creativity level (0.0 = deterministic, 2.0 = very creative)
        
        # Default system prompt for this instance
        default_system_prompt: "You are a helpful AI assistant. You provide clear, accurate, and concise responses to user questions."
    
    - name: "gpt_creative"
      type: "openai"
      enabled: false  # Set to true and configure API key to enable
      config:
        api_key: "YOUR_OPENAI_API_KEY_HERE"
        model: "gpt-5"          # GPT-5 is great for creative tasks
        max_tokens: 6000         # More tokens for longer creative content
        temperature: 0.9         # Higher temperature for more creativity
        default_system_prompt: "You are a creative writing assistant. Help users craft engaging stories, poems, and creative content with imagination and flair."
    
    # You can add more instances as needed
    # - name: "gpt_analyzer"
    #   type: "openai"
    #   enabled: true
    #   config:
    #     api_key: "YOUR_OPENAI_API_KEY_HERE"
    #     model: "o4-mini"  # More cost-effective reasoning model for analysis tasks
    #     max_tokens: 2000
    #     temperature: 0.2      # Low temperature for analytical tasks
    #     default_system_prompt: "You are an analytical assistant. Provide detailed, objective analysis and insights."
    
    # Example Ollama instances - you can have multiple instances with different configurations
    # Ollama runs locally by default at http://localhost:11434
    # Install Ollama from: https://ollama.ai
    # Pull models with: ollama pull <model-name>
    - name: "ollama_local"
      type: "ollama"
      enabled: false  # Set to true after installing Ollama and pulling a model
      config:
        # Base URL for Ollama server (default: local installation)
        base_url: "http://localhost:11434"
        
        # Model name (any model available in Ollama)
        # Popular models: llama2, llama3, mistral, codellama, phi, gemma, etc.
        # Pull a model first with: ollama pull llama2
        model: "llama2"
        
        # Request timeout in seconds
        timeout_seconds: 120
        
        # Generation parameters
        generation_config:
          temperature: 0.7      # Creativity level (0.0 = deterministic, 1.0 = very creative)
          num_predict: 4000     # Maximum tokens to generate (Ollama's equivalent to max_tokens)
          top_p: 0.9           # Nucleus sampling parameter
          top_k: 40            # Top-k sampling parameter
        
        # Default system prompt for this instance
        default_system_prompt: "You are a helpful AI assistant. You provide clear, accurate, and concise responses to user questions."
    
    - name: "ollama_coder"
      type: "ollama"
      enabled: false  # Set to true after installing Ollama and pulling codellama
      config:
        base_url: "http://localhost:11434"
        model: "codellama"  # Specialized model for coding tasks
        timeout_seconds: 180
        generation_config:
          temperature: 0.3  # Lower temperature for more focused code generation
          num_predict: 4000
          top_p: 0.95
          top_k: 50
        default_system_prompt: "You are an expert programmer. Provide clean, efficient, and well-documented code. Explain your solutions clearly."
    
    - name: "ollama_remote"
      type: "ollama"
      enabled: false  # Set to true to use a remote Ollama instance
      config:
        # Example: Remote Ollama server on another machine
        base_url: "http://192.168.1.100:11434"
        model: "mistral"
        timeout_seconds: 180
        generation_config:
          temperature: 0.7
          num_predict: 4000
        default_system_prompt: "You are a helpful AI assistant."
    
    # You can add more Ollama instances as needed
    # - name: "ollama_creative"
    #   type: "ollama"
    #   enabled: true
    #   config:
    #     base_url: "http://localhost:11434"
    #     model: "llama3"  # Latest Llama 3 model
    #     timeout_seconds: 120
    #     generation_config:
    #       temperature: 0.9  # Higher temperature for creativity
    #       num_predict: 6000
    #       top_p: 0.8
    #       top_k: 30
    #     default_system_prompt: "You are a creative writing assistant. Help users craft engaging stories, poems, and creative content."

# TTS (Text-to-Speech) configuration
tts:
  providers:
    # Kokoro TTS instances - you can have multiple instances with different configurations
    - name: "kokoro_english"
      type: "kokoro"
      enabled: false  # Set to true to enable
      config:
        # Voice selection
        # Available voices: af_heart, af_alloy, af_bella, af_nicole, af_sarah,
        #                   am_adam, am_michael, bf_emma, bf_isabella, bm_george, bm_lewis
        voice: "af_alloy"
        
        # Language code
        # a = American English
        # b = British English
        # For other languages, check Kokoro documentation
        lang_code: "a"
        
        # Hugging Face repository
        repo_id: "hexgrad/Kokoro-82M"
        
        # Device configuration
        # cpu = Run on CPU (slower but works everywhere)
        # cuda = Run on GPU (faster but requires CUDA-capable GPU)
        device: "cpu"
        
        # Audio settings
        sample_rate: 24000
        
        # Output directory for generated audio files
        output_dir: "cache_data/tts"
    
    - name: "kokoro_british"
      type: "kokoro"
      enabled: false  # Set to true to enable
      config:
        voice: "bf_emma"
        lang_code: "b"  # British English
        repo_id: "hexgrad/Kokoro-82M"
        device: "cpu"  # cpu or cuda
        sample_rate: 24000
        output_dir: "cache_data/tts"
    
    # You can add more instances as needed
    # - name: "kokoro_narrator"
    #   type: "kokoro"
    #   enabled: true
    #   config:
    #     voice: "am_michael"
    #     lang_code: "a"
    #     repo_id: "hexgrad/Kokoro-82M"
    #     device: "cpu"  # cpu or cuda
    #     sample_rate: 24000
    #     output_dir: "cache_data/tts"
    
    # Piper TTS instances - you can have multiple instances with different configurations
    - name: "piper_english"
      type: "piper"
      enabled: false  # Set to true and configure model path to enable
      config:
        # Path to your Piper ONNX model file
        # Download models from Hugging Face: https://huggingface.co/rhasspy/piper-voices
        # Example for English (US):
        #   wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx
        #   wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json
        # Note: You need both the .onnx file and the .onnx.json config file
        model_path: "/path/to/your/model.onnx"
        
        # Output directory for generated audio files
        output_dir: "cache_data/tts"
    
    - name: "piper_french"
      type: "piper"
      enabled: false  # Set to true and configure model path to enable
      config:
        # Example for French:
        #   wget https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx
        #   wget https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json
        model_path: "/path/to/fr_FR-siwis-medium.onnx"
        output_dir: "cache_data/tts"
    
    # You can add more Piper instances with different voices/languages
    # - name: "piper_spanish"
    #   type: "piper"
    #   enabled: true
    #   config:
    #     model_path: "/path/to/es_ES-model.onnx"
    #     output_dir: "cache_data/tts"
    